https://medium.com/@s_van_laar/deploy-a-private-ipfs-network-on-ubuntu-in-5-steps-5aad95f7261b

https://medium.com/rahasak/ipfs-cluster-with-docker-db2ec20a6cc1

https://github.com/ipfs/ipfs-cluster/issues/612

https://github.com/hsanjuan/ansible-ipfs-cluster/tree/master/host_vars

https://gitlab.com/rahasak-labs/ipfs-ops/-/blob/master/ipfs-cluster-ops/docker-compose.yml

https://labs.eleks.com/2019/03/ipfs-network-data-replication.html

The Cluster peer communicates with the IPFS daemon using the HTTP API (localhost:5001). Therefore, the IPFS daemon must be launched and running separately.

Gateway
The Gateway address is the address that the daemon will serve the gateway interface from. The gateway may be used to view files through IPFS, and serve static web content. This port may or may not be dialable from outside of your machine; that's entirely up to you. The gateway address is optional; if you leave it blank, the gateway server will not start.

**NB:\*\*** we'll be running the gateway on only one node

Cluster peers form an separate, isolated libp2p [private] network, which uses the cluster_secret (a 32-bit hex-encoded passphrase present in the configuration of every peer).

This means that Cluster peers will normally need their own bootstrappers (it can be any peer in the Cluster), although sometimes they can rely on mDNS discovery.

This also means that Cluster peers operate separately from IPFS with regards to NAT hole punching, ports etc.

The shared state: consensus
All peers in the Cluster maintain a global pinset. Making every peer maintain the same view of the pinset regardless of concurrent pinning operations and on a distributed application layout requires coordination given by what is called a consensus component. Cluster supports two implementations:

A CRDT-based approach, based on Conflict-Free Replicated Datatypes
A Raft-based approach, based on a popular log-based consensus algorithm
The relevant details and trade-offs between them are outlined in the Consensus Components section. The choice (which must be performed during initialization and cannot be easily changed), heavily affects the procedures for adding, removing and handling failures in Cluster peers.

The shared state can be inspected with ipfs-cluster-ctl pin ls and is the only piece of information present locally in every peer. Pin status (status) information, or peers information (peers ls) for other than the peer that is running the command, must be obtained at runtime from their respective peers and assembled together

# Initialization

The new service.json file generated by ipfs-cluster-service init will have a randomly generated secret value in the cluster section. For a Cluster to work, this value should be the same in all cluster peers. This is usually a source of pitfalls since initializing default configurations everywhere results in different random secrets.

If present, the CLUSTER_SECRET environment value is used when running ipfs-cluster-service init to set the cluster secret value.

# Remote configuration

ipfs-cluster-service can be initialized to use a remote configuration file accessible on an HTTP(s) location which is read to obtain the running configuration every time the peer is launched. This is useful to initialize all peers with the same configuration and provide seamless upgrades to it.

A good trick is to use IPFS to store the actual configuration and, for example, call init with a gateway url as follows:

$ ipfs-cluster-service init http://localhost:8080/ipns/config.mydomain.com

# Remote configuration

ipfs-cluster-service can be initialized to use a remote configuration file accessible on an HTTP(s) location which is read to obtain the running configuration every time the peer is launched. This is useful to initialize all peers with the same configuration and provide seamless upgrades to it.

A good trick is to use IPFS to store the actual configuration and, for example, call init with a gateway url as follows:

$ ipfs-cluster-service init http://localhost:8080/ipns/config.mydomain.com

# Ports overview

Cluster swarm: tcp:9096 is used by the Cluster swarm and protected by the shared secret. It is OK to expose this port (the cluster secret acts as password to interact with it).

HTTP API: tcp:9094 can be exposed when enabling SSL and setting up basic authentication
libp2p-HTTP API: when using an alternative libp2p host, for the api, the libp2p_listen_multiaddress can be exposed when basic authentication is enabled.
IPFS API: tcp:5001 is the API of the IPFS daemon and should not be exposed to other than localhost.

IPFS Proxy endpoint: tcp:9095 should not be exposed without an authentication mechanism on top (nginx etc…). By default it provides no authentication nor encryption (similar to IPFS’s tcp:5001)

Creating the client secret:
If you have hexdump then:

`od -vN 32 -An -tx1 /dev/urandom | tr -d ' \n' ; echo`
